{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mE6s4kqxWo6I"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer, RobertaTokenizerFast, RobertaForSequenceClassification, BertForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification, XLNetForSequenceClassification, XLNetTokenizerFast\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iODuhML8Wru4"
      },
      "outputs": [],
      "source": [
        "def fine_tune_plm(gpu_numbers: str, seed: int, feature: str, save_model_path: str):\n",
        "    \"\"\"\n",
        "    Fine-tunes a pre-trained language model (PLM) for sequence classification on a specific feature.\n",
        "    Args:\n",
        "        gpu_numbers (str): Comma-separated string of GPU numbers to use.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "        feature (str): The target feature/label column in the dataset (e.g., 'hawkish', 'forward_looking').\n",
        "        save_model_path (str): Path to save the fine-tuned model and tokenizer.\n",
        "    Returns:\n",
        "        list: Experiment results including training and testing metrics.\n",
        "    \"\"\"\n",
        "    # GPU setup\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_numbers\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    # Load the dataset\n",
        "    data = pd.read_csv(\"data.csv\")  # Adjust file path as needed\n",
        "    # print(\"Dataset columns:\", data.columns)  # Debugging: print columns\n",
        "\n",
        "    # Ensure the feature exists\n",
        "    if feature not in data.columns:\n",
        "        raise ValueError(f\"Feature '{feature}' not found in dataset columns: {data.columns}\")\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    data[feature] = label_encoder.fit_transform(data[feature])\n",
        "\n",
        "    # Split data\n",
        "    train_data, temp_data = train_test_split(data, test_size=0.2, random_state=seed)\n",
        "    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=seed)\n",
        "\n",
        "    # Select a pre-trained model and tokenizer\n",
        "    num_labels = len(data[feature].unique())\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=num_labels)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Preprocessing\n",
        "    def preprocess_data(dataset, feature):\n",
        "        \"\"\"\n",
        "        Preprocesses the dataset for tokenization.\n",
        "        \"\"\"\n",
        "        # print(f\"Processing feature: {feature}\")  # Debugging\n",
        "        # print(dataset.head())  # Debugging\n",
        "\n",
        "        # Ensure feature exists in the dataset\n",
        "        if feature not in dataset.columns:\n",
        "            raise ValueError(f\"Feature '{feature}' not found in dataset columns: {dataset.columns}\")\n",
        "\n",
        "        sentences = dataset[\"sentences\"].tolist()  # Replace \"sentences\" with the text column\n",
        "        labels = dataset[feature].tolist()\n",
        "        tokens = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        return TensorDataset(tokens['input_ids'], tokens['attention_mask'], torch.LongTensor(labels))\n",
        "\n",
        "    # Preprocess datasets\n",
        "    train_dataset = preprocess_data(train_data, feature)\n",
        "    val_dataset = preprocess_data(val_data, feature)\n",
        "    test_dataset = preprocess_data(test_data, feature)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "    # TODO: Define optimizer\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': model.roberta.parameters(), 'lr': 1e-5},\n",
        "        {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
        "    ])\n",
        "\n",
        "    # Training and Validation Loop\n",
        "    max_num_epochs = 20\n",
        "    early_stopping_count = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(max_num_epochs):\n",
        "        # TODO: Implement training logic\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            batch = [b.to(device) for b in batch]\n",
        "            inputs, masks, labels = batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # TODO: Implement validation logic\n",
        "        model.eval()\n",
        "        val_loss, val_f1, val_precision, val_recall = 0, 0, 0, 0\n",
        "        for batch in val_dataloader:\n",
        "            batch = [b.to(device) for b in batch]\n",
        "            inputs, masks, labels = batch\n",
        "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            # TODO: Compute F1, precision, and recall for validation\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            val_f1 += f1_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=0)\n",
        "            val_precision += precision_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=0)\n",
        "            val_recall += recall_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=0)\n",
        "\n",
        "        val_loss /= len(val_dataloader)  # Average validation loss\n",
        "        val_f1 /= len(val_dataloader)   # Average F1 score\n",
        "        val_precision /= len(val_dataloader)  # Average precision\n",
        "        val_recall /= len(val_dataloader)  # Average recall\n",
        "        print(f\"Validation Loss: {val_loss}, F1: {val_f1}, Precision: {val_precision}, Recall: {val_recall}\")\n",
        "\n",
        "        # TODO: Update early stopping counter\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            early_stopping_count = 0\n",
        "            torch.save({'model_state_dict': model.state_dict()}, 'best_model.pt')\n",
        "        else:\n",
        "            early_stopping_count += 1\n",
        "\n",
        "        # TODO:\n",
        "        # Add logging/print statements to monitor training progress\n",
        "\n",
        "        # Break if early stopping condition is met\n",
        "        if early_stopping_count >= 5:\n",
        "            break\n",
        "\n",
        "    # TODO: Load the best model\n",
        "    checkpoint = torch.load('best_model.pt')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Testing\n",
        "    # Uncomment the following line once the model is loaded\n",
        "    model.eval()\n",
        "    test_loss, test_accuracy, test_f1, test_precision, test_recall = 0, 0, 0, 0, 0  # Initialize test metrics\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # TODO: Implement test evaluation\n",
        "        for batch in test_dataloader:\n",
        "            batch = [b.to(device) for b in batch]\n",
        "            inputs, masks, labels = batch\n",
        "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "            test_loss += outputs.loss.item()\n",
        "            # TODO: Compute F1, precision, and recall for test evaluation\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            test_accuracy += accuracy_score(labels.cpu(), preds.cpu())\n",
        "            test_f1 += f1_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=0)\n",
        "            test_precision += precision_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=0)\n",
        "            test_recall += recall_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=0)\n",
        "\n",
        "        pass\n",
        "\n",
        "    # TODO: Compute average test loss, accuracy, F1, precision, and recall\n",
        "    test_loss /= len(test_dataloader)\n",
        "    test_accuracy /= len(test_dataloader)\n",
        "    test_f1 /= len(test_dataloader)\n",
        "    test_precision /= len(test_dataloader)\n",
        "    test_recall /= len(test_dataloader)\n",
        "\n",
        "    print(f\"Test Metrics: Loss: {test_loss}, Accuracy: {test_accuracy}, F1: {test_f1}, Precision: {test_precision}, Recall: {test_recall}\")\n",
        "\n",
        "    # Save model\n",
        "    if save_model_path:\n",
        "        model.save_pretrained(save_model_path)\n",
        "        tokenizer.save_pretrained(save_model_path)\n",
        "        pass\n",
        "\n",
        "    return [seed, feature, test_loss, test_accuracy, test_f1, test_precision, test_recall]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Z4t3iQrWW1LI"
      },
      "outputs": [],
      "source": [
        "def fine_tune_deberta(gpu_numbers: str, seed: int, feature: str, save_model_path: str):\n",
        "    \"\"\"\n",
        "    Fine-tunes the DeBERTa language model (microsoft/deberta-v3-large) for sequence classification on a specific feature.\n",
        "    Args:\n",
        "        gpu_numbers (str): Comma-separated string of GPU numbers to use.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "        feature (str): The target feature/label column in the dataset (e.g., 'hawkish', 'forward_looking').\n",
        "        save_model_path (str): Path to save the fine-tuned model and tokenizer.\n",
        "    Returns:\n",
        "        list: Experiment results including training and testing metrics.\n",
        "    \"\"\"\n",
        "    # GPU setup\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_numbers\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    # Load the dataset\n",
        "    data = pd.read_csv(\"data.csv\")  # Adjust file path as needed\n",
        "\n",
        "    # Ensure the feature exists\n",
        "    if feature not in data.columns:\n",
        "        raise ValueError(f\"Feature '{feature}' not found in dataset columns: {data.columns}\")\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    data[feature] = label_encoder.fit_transform(data[feature])\n",
        "\n",
        "    # Split data\n",
        "    train_data, temp_data = train_test_split(data, test_size=0.2, random_state=seed)\n",
        "    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=seed)\n",
        "\n",
        "    # Select a pre-trained model and tokenizer\n",
        "    num_labels = len(data[feature].unique())\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\", num_labels=num_labels)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Preprocessing\n",
        "    def preprocess_data(dataset, feature):\n",
        "        \"\"\"\n",
        "        Preprocesses the dataset for tokenization.\n",
        "        \"\"\"\n",
        "        sentences = dataset[\"sentences\"].tolist()  # Replace \"sentences\" with the text column\n",
        "        labels = dataset[feature].tolist()\n",
        "        tokens = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        return TensorDataset(tokens['input_ids'], tokens['attention_mask'], torch.LongTensor(labels))\n",
        "\n",
        "    # Preprocess datasets\n",
        "    train_dataset = preprocess_data(train_data, feature)\n",
        "    val_dataset = preprocess_data(val_data, feature)\n",
        "    test_dataset = preprocess_data(test_data, feature)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': model.deberta.parameters(), 'lr': 1e-5},\n",
        "        {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
        "    ])\n",
        "\n",
        "    # Training and Validation Loop\n",
        "    max_num_epochs = 20\n",
        "    early_stopping_count = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(max_num_epochs):\n",
        "        # Training logic\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            batch = [b.to(device) for b in batch]\n",
        "            inputs, masks, labels = batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation logic\n",
        "        model.eval()\n",
        "        val_loss, val_f1, val_precision, val_recall = 0, 0, 0, 0\n",
        "        for batch in val_dataloader:\n",
        "            batch = [b.to(device) for b in batch]\n",
        "            inputs, masks, labels = batch\n",
        "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            val_f1 += f1_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=0)\n",
        "            val_precision += precision_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=0)\n",
        "            val_recall += recall_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=0)\n",
        "\n",
        "        val_loss /= len(val_dataloader)\n",
        "        val_f1 /= len(val_dataloader)\n",
        "        val_precision /= len(val_dataloader)\n",
        "        val_recall /= len(val_dataloader)\n",
        "        print(f\"Validation Loss: {val_loss}, F1: {val_f1}, Precision: {val_precision}, Recall: {val_recall}\")\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            early_stopping_count = 0\n",
        "            torch.save({'model_state_dict': model.state_dict()}, 'deberta_best_model.pt')\n",
        "        else:\n",
        "            early_stopping_count += 1\n",
        "\n",
        "        if early_stopping_count >= 5:\n",
        "            break\n",
        "\n",
        "    # Load the best model\n",
        "    checkpoint = torch.load('deberta_best_model.pt')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Testing\n",
        "    model.eval()\n",
        "    test_loss, test_accuracy, test_f1, test_precision, test_recall = 0, 0, 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            batch = [b.to(device) for b in batch]\n",
        "            inputs, masks, labels = batch\n",
        "            outputs = model(inputs, attention_mask=masks, labels=labels)\n",
        "            test_loss += outputs.loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            test_accuracy += accuracy_score(labels.cpu(), preds.cpu())\n",
        "            test_f1 += f1_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=0)\n",
        "            test_precision += precision_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=0)\n",
        "            test_recall += recall_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=0)\n",
        "\n",
        "    test_loss /= len(test_dataloader)\n",
        "    test_accuracy /= len(test_dataloader)\n",
        "    test_f1 /= len(test_dataloader)\n",
        "    test_precision /= len(test_dataloader)\n",
        "    test_recall /= len(test_dataloader)\n",
        "\n",
        "    print(f\"Test Metrics: Loss: {test_loss}, Accuracy: {test_accuracy}, F1: {test_f1}, Precision: {test_precision}, Recall: {test_recall}\")\n",
        "\n",
        "    if save_model_path:\n",
        "        model.save_pretrained(save_model_path)\n",
        "        tokenizer.save_pretrained(save_model_path)\n",
        "\n",
        "    return [seed, feature, test_loss, test_accuracy, test_f1, test_precision, test_recall]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_metrics_to_csv(metrics: list, output_file: str):\n",
        "    \"\"\"\n",
        "    Save metrics to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        metrics (list): List of dictionaries containing model evaluation metrics.\n",
        "        output_file (str): Path to save the CSV file.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(metrics)  # Convert list of dictionaries to DataFrame\n",
        "    df.to_csv(output_file, index=False)  # Save to CSV without an index\n",
        "    print(f\"Metrics saved to {output_file}\")"
      ],
      "metadata": {
        "id": "l8Kkm6jn_h-m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_numbers = \"1\" # Depending on the number of GPUs available. 0 for single GPU, \"0,1\" for multiple GPUs, None for CPU\n",
        "seed = 42 # TODO: set the seed for reproducibility\n",
        "save_model_path1 = \"./models/\"\n",
        "features = [\"sentiment_label\", \"time_label\", \"certain_label\"] # TODO: Add the list of features to fine-tune for (e.g. (sentiment_label, time_label, certain_label))\n",
        "all_results = []\n",
        "\n",
        "\n",
        "print('roberta-large results:')\n",
        "for feature in features:\n",
        "    print(f\"Fine-tuning for feature: {feature}\")\n",
        "    results = fine_tune_plm(gpu_numbers=gpu_numbers, seed=seed, feature=feature, save_model_path=save_model_path + feature)\n",
        "    print(f\"Results for {feature}: {results}\")\n",
        "\n",
        "all_results.append({\n",
        "        \"model\": \"roberta-large\",\n",
        "        \"feature\": feature,\n",
        "        \"seed\": seed,\n",
        "        \"test_loss\": results[2],\n",
        "        \"test_accuracy\": results[3],\n",
        "        \"test_f1\": results[4],\n",
        "        \"test_precision\": results[5],\n",
        "        \"test_recall\": results[6],\n",
        "    })\n",
        "\n",
        "output_csv = \"overall_metrics.csv\"\n",
        "save_metrics_to_csv(all_results, output_csv)\n"
      ],
      "metadata": {
        "id": "cPhvwm2PeOfr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}